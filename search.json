[
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Setting Up of Quarto",
    "section": "",
    "text": "Before diving into literate programming with Quarto, ensure that your development environment is properly set up. Follow these steps to install and configure the necessary tools:",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#introduction",
    "href": "setup/index.html#introduction",
    "title": "Setting Up of Quarto",
    "section": "",
    "text": "Before diving into literate programming with Quarto, ensure that your development environment is properly set up. Follow these steps to install and configure the necessary tools:",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#step-1-install-python",
    "href": "setup/index.html#step-1-install-python",
    "title": "Setting Up of Quarto",
    "section": "Step 1: Install Python",
    "text": "Step 1: Install Python\nPython is essential for this workshop as it is integral to Quarto for executing code within documents, accessing a wide range of libraries and tools for data analysis and machine learning, and enabling script execution. It also integrates seamlessly with Visual Studio Code, our recommended IDE, to enhance coding efficiency and support advanced machine learning tasks. By installing Python, you set up a powerful environment that supports both technical documentation and complex computational projects.\n\nDownload and Install Python\n\nGo to the official Python website.\nDownload the latest stable version of Python suitable for your operating system (Windows, macOS, or Linux).\nRun the installer and ensure you check the box to “Add Python to PATH” during installation.\nFollow the on-screen instructions to complete the installation.",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#step-2-install-visual-studio-code-vs-code",
    "href": "setup/index.html#step-2-install-visual-studio-code-vs-code",
    "title": "Setting Up of Quarto",
    "section": "Step 2: Install Visual Studio Code (VS Code)",
    "text": "Step 2: Install Visual Studio Code (VS Code)\n\nDownload and Install Visual Studio Code (VS Code)\n\nVisit the official VS Code website.\nDownload the installer for your operating system.\nRun the installer and follow the prompts to install VS Code.\nAfter installation, open VS Code and install the recommended extensions:\n\nPython Extension: Search for “Python” in the Extensions view (Ctrl+Shift+X) and install it.\nQuarto Extension: Search for “Quarto” in the Extensions view and install it.",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#step-3-install-quarto-cli",
    "href": "setup/index.html#step-3-install-quarto-cli",
    "title": "Setting Up of Quarto",
    "section": "Step 3: Install Quarto CLI",
    "text": "Step 3: Install Quarto CLI\nQuarto CLI is crucial as it allows you to create, render, and manage Quarto documents and projects. Quarto CLI integrates seamlessly with your development environment, enabling you to compile literate programming documents that combine code and narrative effectively. It supports various output formats, including HTML, PDF, and slides, and facilitates the integration of code with documentation. By installing Quarto CLI, you equip yourself with the necessary tools to manage and execute Quarto projects efficiently, making it a key component of your setup for this workshop.\n\nDownload and Install Quarto CLI\n\nVisit the Quarto CLI download page.\nDownload the latest stable release of Quarto CLI for your operating system.\nRun the installer and follow the on-screen instructions to complete the installation.\nAfter installation, verify that Quarto CLI is correctly installed by opening a command line interface (Terminal on macOS/Linux, Command Prompt or PowerShell on Windows) and typing:\nquarto --version",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#step-4-install-git-for-cicd",
    "href": "setup/index.html#step-4-install-git-for-cicd",
    "title": "Setting Up of Quarto",
    "section": "Step 4: Install Git for CI/CD",
    "text": "Step 4: Install Git for CI/CD\nGit is essential for version control and collaborative development, enabling you to track changes in your code, manage different versions of your projects, and collaborate effectively with others. It supports continuous integration and deployment (CI/CD) workflows, which are crucial for maintaining and deploying code systematically. Additionally, Git integrates with GitHub, allowing you to host and publish websites using GitHub Pages, providing a seamless way to share and showcase your projects online. By installing Git, you ensure that you can manage and synchronize your work efficiently, maintain a history of changes, collaborate seamlessly, and leverage GitHub Pages for web hosting throughout the workshop.\n\nDownload and Install Git\n\nVisit the official Git website.\nDownload the installer for your operating system.\nRun the installer and follow the prompts. You can use the default settings for most options.\nEnsure Git is added to your system PATH by checking the appropriate option during installation.\n\nConfigure Git\n\nOpen your command line interface (Terminal on macOS/Linux, Command Prompt or PowerShell on Windows).\nConfigure your Git username and email:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\nVerify the installation by checking the Git version:\ngit --version\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are in a fresh Python 3 environment, installing the jupyter package will provide everything required to execute Jupyter kernels with Quarto:\npython3 -m pip install jupyter\n\n\nThe publishing workflow is the process in which external resources are prepared and collated together with the help of an authoring tool and subsequently rendered with a formatting tool to generate a publishable output that can be delivered in various forms. This is visualised by the following figure.",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#literate-programming-workflow",
    "href": "setup/index.html#literate-programming-workflow",
    "title": "Setting Up of Quarto",
    "section": "Literate Programming Workflow",
    "text": "Literate Programming Workflow\n\n\n\n\n\n---\n Literate Programming Workflow\n---\ngraph TD\n    A[Python] --&gt;|Code Execution| B[Quarto CLI]\n    B --&gt;|Document Rendering| C[Technical Documents]\n    B --&gt;|Document Rendering| D[Blogs]\n    B --&gt;|Document Rendering| E[Websites]\n    E --&gt;|Host| F[GitHub Pages]\n    G[Git] --&gt;|Version Control| C\n    G --&gt;|Version Control| D\n    G --&gt;|Version Control| E\n    G --&gt;|CI/CD| F\n\n    subgraph Development\n        A\n        B\n        G\n    end\n\n    subgraph Output\n        C\n        D\n        E\n        F\n    end\n\n    style Development fill:#f9f,stroke:#333,stroke-width:2px\n    style Output fill:#ccf,stroke:#333,stroke-width:2px",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#creating-your-personal-website",
    "href": "setup/index.html#creating-your-personal-website",
    "title": "Setting Up of Quarto",
    "section": "Creating Your Personal Website",
    "text": "Creating Your Personal Website\nIn the digital era, a personal website is an essential tool for engineering students looking to showcase their skills, projects, and achievements. It acts as an online portfolio, a platform for sharing insights, and a way to make a professional impression. Creating your own website involves several key steps, from planning and design to development and hosting. This guide will walk you through the process of designing a user-friendly site, building it with the right tools, testing and optimizing its performance, and finally, hosting it for the world to see. Whether you want to present your academic projects, share your resume, or maintain a blog, a well-crafted personal website can enhance your visibility and open up new opportunities in your engineering career.",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#step-1-clone-the-website-template-respository-from-github",
    "href": "setup/index.html#step-1-clone-the-website-template-respository-from-github",
    "title": "Setting Up of Quarto",
    "section": "Step 1: Clone the Website template respository from Github",
    "text": "Step 1: Clone the Website template respository from Github\nCopy the following Github repo url and clone using Vscode source control.\nhttps://github.com/sijuswamy/Website-Template",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/index.html#step-2-open-the-folder-in-the-vscode-and-change-and-update-the-necessary-details-and-content.",
    "href": "setup/index.html#step-2-open-the-folder-in-the-vscode-and-change-and-update-the-necessary-details-and-content.",
    "title": "Setting Up of Quarto",
    "section": "Step 2: Open the folder in the VScode and change and update the necessary details and content.",
    "text": "Step 2: Open the folder in the VScode and change and update the necessary details and content.\nYou can author Quarto documents that include Python code using any text or notebook editor. No matter what editing tool you use, you’ll always run quarto preview first to setup a live preview of changes in your document. Live preview is available for both HTML and PDF output. For example:\nUse any of the following options to create your personal website designed in Quarto.\n# preview as html\nquarto preview index.qmd\n\n# preview as pdf\nquarto preview index.qmd --to pdf\n\n# preview a jupyter notebook\nquarto preview index.ipynb\n\nRendering\nYou can use quarto render command line options to control caching behavior without changing the document’s code. Use options to force the use of caching on all chunks, disable the use of caching on all chunks (even if it’s specified in options), or to force a refresh of the cache even if it has not been invalidated:\n# use a cache (even if not enabled in options)\nquarto render example.qmd --cache \n\n# don't use a cache (even if enabled in options)\nquarto render example.qmd --no-cache \n\n# use a cache and force a refresh \nquarto render example.qmd --cache-refresh",
    "crumbs": [
      "Home",
      "Setting-up"
    ]
  },
  {
    "objectID": "setup/slides-quarto.html#step-1-install-python",
    "href": "setup/slides-quarto.html#step-1-install-python",
    "title": "Setting Up of Quarto",
    "section": "Step 1: Install Python",
    "text": "Step 1: Install Python\n\nDownload and Install Python\n\nGo to the official Python website.\nDownload the latest stable version of Python suitable for your operating system (Windows, macOS, or Linux).\nRun the installer and ensure you check the box to “Add Python to PATH” during installation.\nFollow the on-screen instructions to complete the installation."
  },
  {
    "objectID": "setup/slides-quarto.html#step-2-install-visual-studio-code-vs-code",
    "href": "setup/slides-quarto.html#step-2-install-visual-studio-code-vs-code",
    "title": "Setting Up of Quarto",
    "section": "Step 2: Install Visual Studio Code (VS Code)",
    "text": "Step 2: Install Visual Studio Code (VS Code)\n\nDownload and Install Visual Studio Code (VS Code)\n\nVisit the official VS Code website.\nDownload the installer for your operating system.\nRun the installer and follow the prompts to install VS Code.\nAfter installation, open VS Code and install the recommended extensions:\n\nPython Extension: Search for “Python” in the Extensions view (Ctrl+Shift+X) and install it.\nQuarto Extension: Search for “Quarto” in the Extensions view and install it."
  },
  {
    "objectID": "setup/slides-quarto.html#step-3-install-quarto-cli",
    "href": "setup/slides-quarto.html#step-3-install-quarto-cli",
    "title": "Setting Up of Quarto",
    "section": "Step 3: Install Quarto CLI",
    "text": "Step 3: Install Quarto CLI\n\nDownload and Install Quarto CLI\n\nVisit the Quarto CLI download page.\nDownload the latest stable release of Quarto CLI for your operating system.\nRun the installer and follow the on-screen instructions to complete the installation.\nAfter installation, verify that Quarto CLI is correctly installed by opening a command line interface (Terminal on macOS/Linux, Command Prompt or PowerShell on Windows) and typing:\nquarto --version"
  },
  {
    "objectID": "setup/slides-quarto.html#step-4-install-git-for-cicd",
    "href": "setup/slides-quarto.html#step-4-install-git-for-cicd",
    "title": "Setting Up of Quarto",
    "section": "Step 4: Install Git for CI/CD",
    "text": "Step 4: Install Git for CI/CD\n\nDownload and Install Git\n\nVisit the official Git website.\nDownload the installer for your operating system.\nRun the installer and follow the prompts. You can use the default settings for most options.\nEnsure Git is added to your system PATH by checking the appropriate option during installation.\n\nConfigure Git\n\nOpen your command line interface (Terminal on macOS/Linux, Command Prompt or PowerShell on Windows).\nConfigure your Git username and email:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\nVerify the installation by checking the Git version:\ngit --version"
  },
  {
    "objectID": "setup/slides-quarto.html#literate-programming-workflow",
    "href": "setup/slides-quarto.html#literate-programming-workflow",
    "title": "Setting Up of Quarto",
    "section": "Literate Programming Workflow",
    "text": "Literate Programming Workflow\n\n\n\n\n\n---\n Literate Programming Workflow\n---\ngraph TD\n    A[Python] --&gt;|Code Execution| B[Quarto CLI]\n    B --&gt;|Document Rendering| C[Technical Documents]\n    B --&gt;|Document Rendering| D[Blogs]\n    B --&gt;|Document Rendering| E[Websites]\n    E --&gt;|Host| F[GitHub Pages]\n    G[Git] --&gt;|Version Control| C\n    G --&gt;|Version Control| D\n    G --&gt;|Version Control| E\n    G --&gt;|CI/CD| F\n\n    subgraph Development\n        A\n        B\n        G\n    end\n\n    subgraph Output\n        C\n        D\n        E\n        F\n    end\n\n    style Development fill:#f9f,stroke:#333,stroke-width:2px\n    style Output fill:#ccf,stroke:#333,stroke-width:2px"
  },
  {
    "objectID": "setup/slides-quarto.html#creating-your-personal-website",
    "href": "setup/slides-quarto.html#creating-your-personal-website",
    "title": "Setting Up of Quarto",
    "section": "Creating Your Personal Website",
    "text": "Creating Your Personal Website"
  },
  {
    "objectID": "setup/slides-quarto.html#step-1-clone-the-website-template-respository-from-github",
    "href": "setup/slides-quarto.html#step-1-clone-the-website-template-respository-from-github",
    "title": "Setting Up of Quarto",
    "section": "Step 1: Clone the Website template respository from Github",
    "text": "Step 1: Clone the Website template respository from Github\nCopy the following Github repo url and clone using Vscode source control.\nhttps://github.com/sijuswamy/Website-Template"
  },
  {
    "objectID": "setup/slides-quarto.html#step-2-open-the-folder-in-the-vscode-and-change-and-update-the-necessary-details-and-content.",
    "href": "setup/slides-quarto.html#step-2-open-the-folder-in-the-vscode-and-change-and-update-the-necessary-details-and-content.",
    "title": "Setting Up of Quarto",
    "section": "Step 2: Open the folder in the VScode and change and update the necessary details and content.",
    "text": "Step 2: Open the folder in the VScode and change and update the necessary details and content.\nUse any of the following options to create your personal website designed in Quarto.\n# preview as html\nquarto preview index.qmd\n\n# preview as pdf\nquarto preview index.qmd --to pdf\n\n# preview a jupyter notebook\nquarto preview index.ipynb\nRendering\nYou can use quarto render command line options to control caching behavior without changing the document’s code. Use options to force the use of caching on all chunks, disable the use of caching on all chunks (even if it’s specified in options), or to force a refresh of the cache even if it has not been invalidated:\n# use a cache (even if not enabled in options)\nquarto render example.qmd --cache \n\n# don't use a cache (even if enabled in options)\nquarto render example.qmd --no-cache \n\n# use a cache and force a refresh \nquarto render example.qmd --cache-refresh"
  },
  {
    "objectID": "introduction/index.html",
    "href": "introduction/index.html",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "",
    "text": "Introduction\nWelcome to Day 1 of our workshop! Today, we embark on an exploration of literate programming through the powerful open-source tool, Quarto. Literate programming is a methodology that integrates code and documentation, allowing for clearer, more maintainable, and more insightful technical writing. By the end of today, you’ll gain hands-on experience with Quarto, learning how to craft high-quality technical documents, build interactive web pages, and create engaging blogs. Our sessions are designed to provide both theoretical knowledge and practical skills, equipping you with the tools to effectively communicate your code and ideas. Let’s dive into the world of literate programming and see how Quarto can transform your documentation practices.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#day-1-unleashing-literate-programming-with-quarto",
    "href": "introduction/index.html#day-1-unleashing-literate-programming-with-quarto",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "",
    "text": "Introduction\nWelcome to Day 1 of our workshop! Today, we embark on an exploration of literate programming through the powerful open-source tool, Quarto. Literate programming is a methodology that integrates code and documentation, allowing for clearer, more maintainable, and more insightful technical writing. By the end of today, you’ll gain hands-on experience with Quarto, learning how to craft high-quality technical documents, build interactive web pages, and create engaging blogs. Our sessions are designed to provide both theoretical knowledge and practical skills, equipping you with the tools to effectively communicate your code and ideas. Let’s dive into the world of literate programming and see how Quarto can transform your documentation practices.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#getting-started",
    "href": "introduction/index.html#getting-started",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "Getting started",
    "text": "Getting started\n\nGet started documentation: quarto.org/docs/get-started/\nOpen-source repository in GitHub: Quarto-cli\nCreate a project with quarto create project\n\nType: default, website, blog, manuscript, book, confluence\n\nBuild project with quarto render\nPreview with quarto preview (it autobuilds and updates when changes in the source files are detected).",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#integrated-development-environments",
    "href": "introduction/index.html#integrated-development-environments",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "Integrated Development Environments",
    "text": "Integrated Development Environments\nThe separation of the source code and the publishable outputs is something that all Integrated Development Environments (IDEs) provide. These are tools for writing computer programs that commonly require a compilation phase which is usually integrated in the same tool. The idea of authoring tools that can create generic input artefacts that are later combined by a formatting tool is very similar to the common process followed in programming compiled programming languages. This has facilitate the adoption of IDEs as authoring tools. Microsoft Visual Studio and Posit Workbench (formerly RStudio) have tools to work with the Quarto environment. Both of them provide options for collaborative and contemporaneous editing.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/index.html#key-points-about-quarto",
    "href": "introduction/index.html#key-points-about-quarto",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "Key points about Quarto",
    "text": "Key points about Quarto\n\nQuarto is a formatting tool\nuses pandoc to convert the input artefacts to various outputs.\nsupports plain text markdown, Jupyter notebooks and an augmented markdown,\nsupports dynamic content with Python, R, Julia and Observable programming languages.\nis integrated in multiple IDEs: Visual Studio, Posit Connect (former RMarkdown), Atlassian Confluence, …",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction/slides-quarto.html#getting-started",
    "href": "introduction/slides-quarto.html#getting-started",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "Getting started",
    "text": "Getting started\n\nGet started documentation: quarto.org/docs/get-started/\nOpen-source repository in GitHub: Quarto-cli\nCreate a project with quarto create project\n\nType: default, website, blog, manuscript, book, confluence\n\nBuild project with quarto render\nPreview with quarto preview (it autobuilds and updates when changes in the source files are detected)."
  },
  {
    "objectID": "introduction/slides-quarto.html#integrated-development-environments",
    "href": "introduction/slides-quarto.html#integrated-development-environments",
    "title": "Introduction to Literate Programming with Quarto",
    "section": "Integrated Development Environments",
    "text": "Integrated Development Environments\nQuarto is integrated in multiple IDEs"
  },
  {
    "objectID": "hands_on.html",
    "href": "hands_on.html",
    "title": "Hands-on Session on Quarto",
    "section": "",
    "text": "Introduction to Quarto\nGet Started\nHands-on\n\nListings\nTemplates and Customization\nPrograming",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "hands_on.html#agenda",
    "href": "hands_on.html#agenda",
    "title": "Hands-on Session on Quarto",
    "section": "",
    "text": "Introduction to Quarto\nGet Started\nHands-on\n\nListings\nTemplates and Customization\nPrograming",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "hands_on.html#get-started",
    "href": "hands_on.html#get-started",
    "title": "Hands-on Session on Quarto",
    "section": "Get Started",
    "text": "Get Started\n\nDownloading Quarto\n\nDownload Quarto\n\nLinux\nMacOS\nWindows\n\nChoose your platform\n\nVS Code\nJupyter Notebook/Lab\nNeoVim\nRStudio\n\n\n\n\n\n\nDownload Quarto\n\nDownload Quarto: https://quarto.org/docs/get-started/\nWorkshop website: https://sijuswamy.github.io/CME-workshop-1/\n\n\nSetup\n\nTerminal + Text Editor (VS Code)\nVisual Studio Code has options to use quarto in a better user interface\n\n\n\nRun the command ‘quarto create project website ’\nAlternatively, ctrl+shift+p and create the quarto project\nAutomatically Quarto will create the following directory structure:\n\n_quarto.yml\nindex.qmd\nabout.qmd\nstyles.css\n\n\n\n\nStructure\n\n.yml files\n\nYAML is a human-readable data serialization language\nYAML is an official strict superset of JSON despite looking very different from JSON.\nTo create a YAML file, use either the .yaml or .yml file extension.\n\n.qmd files\n\nWork as markdown files, but they have a configuration section in yml on the top of the file\n\n\n\n\n_quarto.yml\n\nDefines the basic structure of the website.\nAll configurations are done using yml\nSome of the configurations:\n\nNavigation bar\nSide bar\nContents",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "hands_on.html#listings",
    "href": "hands_on.html#listings",
    "title": "Hands-on Session on Quarto",
    "section": "Listings",
    "text": "Listings\n\nListings enable you to automatically generate the contents of a page (or region of a page) from a list of Quarto documents or other custom data\nUseful to create blogs, newletters\nLink to the documentation: https://quarto.org/docs/websites/website-listings.html",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "hands_on.html#templates-and-customization",
    "href": "hands_on.html#templates-and-customization",
    "title": "Hands-on Session on Quarto",
    "section": "Templates and Customization",
    "text": "Templates and Customization\n\nSearch bar\n\nhttps://quarto.org/docs/websites/website-search.html\n\nThemes\n\nhttps://quarto.org/docs/output-formats/html-themes.html\n\nTools\n\nhttps://quarto.org/docs/websites/website-tools.html",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "hands_on.html#programing",
    "href": "hands_on.html#programing",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nQuarto also provides the option to embed some code on your website\nQuarto supports Python, R, Julia and Observable Javascript\nYou can create a code block delimiting using ```\nExample of code running: https://tailor-uob.github.io/training-material/cha_odm/odm.html",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "hands_on.html#programing-1",
    "href": "hands_on.html#programing-1",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nAlso Quarto allows the creation of short codes.\nShortcodes are special markdown directives that generate various types of content. Quarto shortcodes are similar in form and function to Hugo shortcodes and WordPress shortcodes.\nDocumentation: https://quarto.org/docs/extensions/shortcodes.html",
    "crumbs": [
      "Home",
      "Hands-on Session on Quarto"
    ]
  },
  {
    "objectID": "slides.html#agenda",
    "href": "slides.html#agenda",
    "title": "Hands-on Session on Quarto",
    "section": "Agenda",
    "text": "Agenda\n\nIntroduction to Quarto\nGet Started\nHands-on\n\nListings\nTemplates and Customization\nPrograming"
  },
  {
    "objectID": "slides.html#get-started",
    "href": "slides.html#get-started",
    "title": "Hands-on Session on Quarto",
    "section": "Get Started",
    "text": "Get Started\nDownloading Quarto\n\nDownload Quarto\n\nLinux\nMacOS\nWindows\n\nChoose your platform\n\nVS Code\nJupyter Notebook/Lab\nNeoVim\nRStudio"
  },
  {
    "objectID": "slides.html#listings",
    "href": "slides.html#listings",
    "title": "Hands-on Session on Quarto",
    "section": "Listings",
    "text": "Listings\n\nListings enable you to automatically generate the contents of a page (or region of a page) from a list of Quarto documents or other custom data\nUseful to create blogs, newletters\nLink to the documentation: https://quarto.org/docs/websites/website-listings.html"
  },
  {
    "objectID": "slides.html#templates-and-customization",
    "href": "slides.html#templates-and-customization",
    "title": "Hands-on Session on Quarto",
    "section": "Templates and Customization",
    "text": "Templates and Customization\n\nSearch bar\n\nhttps://quarto.org/docs/websites/website-search.html\n\nThemes\n\nhttps://quarto.org/docs/output-formats/html-themes.html\n\nTools\n\nhttps://quarto.org/docs/websites/website-tools.html"
  },
  {
    "objectID": "slides.html#programing",
    "href": "slides.html#programing",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nQuarto also provides the option to embed some code on your website\nQuarto supports Python, R, Julia and Observable Javascript\nYou can create a code block delimiting using ```\nExample of code running: https://tailor-uob.github.io/training-material/cha_odm/odm.html"
  },
  {
    "objectID": "slides.html#programing-1",
    "href": "slides.html#programing-1",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nAlso Quarto allows the creation of short codes.\nShortcodes are special markdown directives that generate various types of content. Quarto shortcodes are similar in form and function to Hugo shortcodes and WordPress shortcodes.\nDocumentation: https://quarto.org/docs/extensions/shortcodes.html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains information about a workshop organised in Saintgits College of Engineering (Autonomous) to introduce the Students of Minor in Computational Mathematics to the new open source publishing system. This is part of the add-on program organized by the Department of Computer Science & Engineering.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "about.html#workshop-organisers",
    "href": "about.html#workshop-organisers",
    "title": "About",
    "section": "Workshop organisers",
    "text": "Workshop organisers\n\n\n\n\n\n\n\n\n\nSiju K S\n\n\n\n\n\n\n\nJobin Jose",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "about.html#sidebar-image",
    "href": "about.html#sidebar-image",
    "title": "About",
    "section": "Sidebar image",
    "text": "Sidebar image\nThe sidebar image was generated by Microsoft Copilot with a prompt, “Transforming Classical Libraries to Digital Libraries with Open source.”",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto Hands on Workshop",
    "section": "",
    "text": "About the Workshop\nWe are pleased to announce that the Department of Computer Science and Engineering, in collaboration with the Department of Mathematics, is hosting a two-day workshop to highlight the transformative power of open-source tools in literate programming and machine learning.\n\n\nAbout Quarto\nThe Quarto publication system is a versatile and powerful tool designed for creating dynamic documents, presentations, websites, and more. It seamlessly integrates text, code, and outputs, making it an ideal choice for data scientists, researchers, and educators who need to combine narrative with analysis. Quarto supports a wide range of programming languages, including R, Python, and Julia, allowing users to embed live code within their documents. Its flexibility extends to output formats as well, enabling the production of HTML, PDF, Word documents, and more, all from a single source file. By leveraging Markdown and advanced features such as citations and cross-referencing, Quarto facilitates the creation of professional and reproducible documents that cater to diverse audiences and purposes.\nJoin us for this hands-on introductory tutorial to the Quarto publishing system. You will learn how to use Quarto to set up a personal academic website, advertise your work, build a blogging environment, as well as create slides, pdf’s or other online materials. At the end of the session you will have the main structure for your personal website, hosted for free in GitHub and with a simple and automatic deployment workflow using GitHub actions. We will provide use case examples and a step-by-step guide to follow along the session.\n\nAgenda\n\n\nDay 1\n\n9:00 Introduction to Literate Programming\n9.10 Testimonials\n9.20 Introduction to Quarto\n9:25 Get Started\n9.50 Break\nHands-on\n\n10:00 Installation and system set-up\n11:00 Website Design\n12.30 Lunch Break\n01.20 Blog Design\n02.30 Hosting Sites\n03:30 Break\n03.45 Personalizing Websites & Blogs\n04.20 Day 1 concluding session\n\n\n\n\n\nDay 2\n\n9:00 Introduction to Machine Learning\n9.50 Break\nHands-on\n\n10:00 Python libraries for Machine Learning\n11:00 Design a Machine Learning Project\n12.30 Lunch Break\n01.20 Story telling with EDA\n02.30 Model selection, training and evaluation\n03:30 Break\n03.45 Building GUI and Model Deployment\n04.20 Feedback session\n\n\n\n\n\n\nReserve your spot.\nPlaces are limited, please register by filling out the form below. The form will be closed when no more spots are available.\nhttps://forms.gle/wyGmPozDT1y3MPuE7\nOr scan the QR Code to register:",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "ML/slides-ML.html#end-to-end-machine-learning-project-classifying-the-iris-dataset",
    "href": "ML/slides-ML.html#end-to-end-machine-learning-project-classifying-the-iris-dataset",
    "title": "End-to-End Machine Learning Project",
    "section": "End-to-End Machine Learning Project: Classifying the Iris Dataset",
    "text": "End-to-End Machine Learning Project: Classifying the Iris Dataset\nIn this project, we will walk through an end-to-end machine learning task using the Iris dataset. This comprehensive exercise will cover all stages of a machine learning pipeline, from data exploration to model deployment."
  },
  {
    "objectID": "ML/slides-ML.html#introduction-to-the-dataset",
    "href": "ML/slides-ML.html#introduction-to-the-dataset",
    "title": "End-to-End Machine Learning Project",
    "section": "1. Introduction to the Dataset",
    "text": "1. Introduction to the Dataset\nThe Iris dataset is a classic dataset in machine learning, widely used for benchmarking classification algorithms. It consists of measurements from 150 iris flowers, with four features- Sepal Length, Sepal Width, Petal Length, and Petal Width. Each sample is labeled with one of three species- Iris-setosa, Iris-versicolor, and Iris-virginica.\nA sample image of iris orcid"
  },
  {
    "objectID": "ML/slides-ML.html#objective",
    "href": "ML/slides-ML.html#objective",
    "title": "End-to-End Machine Learning Project",
    "section": "2. Objective",
    "text": "2. Objective\nOur objective is to build a classification model that can accurately predict the species of an iris flower based on its measurements. We will explore the dataset, perform necessary preprocessing, and select an appropriate classification algorithm to achieve this goal."
  },
  {
    "objectID": "ML/slides-ML.html#data-exploration-and-preprocessing",
    "href": "ML/slides-ML.html#data-exploration-and-preprocessing",
    "title": "End-to-End Machine Learning Project",
    "section": "3. Data Exploration and Preprocessing",
    "text": "3. Data Exploration and Preprocessing\n\nExploratory Data Analysis (EDA): We will begin by analyzing the dataset to understand its structure and characteristics. This includes visualizing distributions, checking for missing values, and examining class balance.\n\nIn this stage we need to load the dataset using appropriate python libraries. We want to follow a systematic approach to understand the dataset’s structure, clean the data, and gain insights. Here’s a step-by-step procedure for EDA using Python. As the first step let’s load necessary python libraries for this job.\nIn this EDA process, libraries such as pandas, seaborn, matplotlib, and scikit-learn are essential. Pandas is used for efficient data manipulation and preprocessing, allowing us to load, clean, and manage the dataset seamlessly. Seaborn and matplotlib provide advanced visualization capabilities to explore the distribution, outliers, and relationships among features, which are crucial for understanding the dataset’s structure and potential issues. Together, these libraries offer a comprehensive toolkit for conducting thorough exploratory data analysis, ensuring that the dataset is well-understood and ready for subsequent modeling.\nIn the next step, we load the Iris dataset directly from a remote URL using pandas. The code iris_df = pd.read_csv('https://raw.githubusercontent.com/sijuswamy/Model_Deployment/main/iris.csv') reads the CSV file from the specified GitHub repository and creates a DataFrame named iris_df, which contains the dataset for further analysis.\nViewing the beggining Dataset: The code iris_df.head() displays the first five rows of the iris_df DataFrame, providing a quick overview of the dataset’s structure and the initial entries. We just visualize first 5 samples in the dataset as a table.\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nSetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nSetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nSetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nSetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nSetosa\n\n\n\n\n\n\n\nViewing the End of the Dataset: The code iris_df.tail() displays the last five rows of the iris_df DataFrame, offering insight into the final entries and the dataset’s structure at its end.\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n145\n6.7\n3.0\n5.2\n2.3\nVirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nVirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nVirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nVirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nVirginica\n\n\n\n\n\n\n\nChecking the Dataset Shape: The code iris_df.shape returns a tuple representing the dimensions of the iris_df DataFrame, indicating the number of rows and columns in the dataset.\n\n\n(150, 5)\n\n\nViewing Column Names: The code iris_df.columns displays the names of all columns in the iris_df DataFrame, providing an overview of the dataset’s features and attributes.\n\n\nIndex(['sepal.length', 'sepal.width', 'petal.length', 'petal.width',\n       'variety'],\n      dtype='object')\n\n\nDataset Information: The code iris_df.info() provides a summary of the iris_df DataFrame, including the number of non-null entries, data types of each column, and memory usage, which helps assess the completeness and structure of the dataset.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal.length  150 non-null    float64\n 1   sepal.width   150 non-null    float64\n 2   petal.length  150 non-null    float64\n 3   petal.width   150 non-null    float64\n 4   variety       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\nStatistical Summary: The code iris_df.describe(include='all') generates a comprehensive summary of the iris_df DataFrame, including statistics for all columns, such as count, unique values, top frequency, and mean, which provides insights into the distribution and characteristics of the dataset.\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150\n\n\nunique\nNaN\nNaN\nNaN\nNaN\n3\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nSetosa\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\n50\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\nNaN\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\nNaN\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\nNaN\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\nNaN\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\nNaN\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\nNaN\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\nNaN\n\n\n\n\n\n\n\nThese basic pandas functions are most important for understand the data well. Now move on to the next level of data preparation namely Data Cleaning.\n\nData Cleaning: We will handle any missing values and ensure the data is ready for modeling. Basic preprocessing tasks will include feature scaling and normalization. Various steps in this stage is explained below.\n\nChecking for Duplicates: The code iris_df.duplicated().sum() counts the number of duplicate rows in the iris_df DataFrame, helping identify any redundancy in the dataset that may need to be addressed.\n\n\n1\n\n\n\n\n\n\n\n\nTip\n\n\nChecking for duplicates is important because duplicate rows can skew analysis, introduce bias, and affect the performance of machine learning models. By identifying and removing duplicates, we ensure that each observation is unique and that the dataset accurately represents the underlying data without redundancy.\n\n\n\nIdentifying Duplicate Rows: The code iris_df[iris_df.duplicated()] filters and displays the duplicate rows in the iris_df DataFrame, allowing us to inspect and address any redundancy in the dataset by showing which rows are duplicated.\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n142\n5.8\n2.7\n5.1\n1.9\nVirginica\n\n\n\n\n\n\n\nChecking Class Distribution and Data Imbalance: The code iris_df['variety'].value_counts() counts the number of occurrences of each unique value in the variety column of the iris_df DataFrame, providing insight into the distribution of classes and helping to identify any class imbalances in the dataset.\n\n\nvariety\nSetosa        50\nVersicolor    50\nVirginica     50\nName: count, dtype: int64\n\n\n\n\n\n\n\n\nCaution\n\n\nAn imbalanced dataset, where some classes are significantly underrepresented compared to others, can lead to biased model performance. It may cause the model to favor the majority class, resulting in poor predictive accuracy for the minority class and skewed evaluation metrics. Addressing class imbalance ensures that the model learns to generalize across all classes effectively, leading to more reliable and fair predictions.\n\n\n\nChecking for Missing Values: The code iris_df.isnull().sum(axis=0) calculates the number of missing values for each column in the iris_df DataFrame, helping to identify and address any gaps in the dataset that may need to be handled before analysis or modeling.\n\n\nsepal.length    0\nsepal.width     0\npetal.length    0\npetal.width     0\nvariety         0\ndtype: int64\n\n\nChecking for missing values is essential because missing data can compromise the integrity of the analysis and modeling process. By identifying columns with missing values, we can take appropriate steps to handle them—such as imputation or removal—ensuring that the dataset is complete and reliable for generating accurate insights and predictions.\nStatistical summary: Checking skewness, kurtosis, and correlation is essential for understanding data distribution and feature relationships. Skewness measures asymmetry; values between -0.5 and 0.5 indicate a fairly normal distribution, while values beyond this range suggest significant skewness. Kurtosis assesses the heaviness of tails; values close to 3 indicate a normal distribution, while values much higher or lower suggest the presence or absence of outliers, respectively. Correlation examines feature relationships, with values close to 1 or -1 indicating strong correlations that could lead to multicollinearity. Analyzing these metrics helps in identifying data transformation needs, managing outliers, and optimizing feature selection, ultimately improving model performance and reliability. Before performing the statistical operations, check for the categorical variables. If so remove them and apply statistical operations on that pruned dataset. The following code will do that.\n\n\nSuccessfully removed 'variety' column.\n\n\nChecking Skewness: The code iris_num.skew() calculates the skewness of each numeric column in the iris_num DataFrame, providing insights into the asymmetry of the data distribution. Skewness values between -0.5 and 0.5 suggest a relatively normal distribution, while values outside this range indicate potential skewness that may require transformation for better modeling.\n\n\nsepal.length    0.314911\nsepal.width     0.318966\npetal.length   -0.274884\npetal.width    -0.102967\ndtype: float64\n\n\nChecking Kurtosis: The code iris_num.kurt() calculates the kurtosis of each numeric column in the iris_num DataFrame, which measures the “tailedness” of the data distribution. Values close to 3 suggest a distribution similar to the normal distribution, while values significantly higher or lower indicate heavy or light tails, respectively, which may point to the presence of outliers or a lack thereof.\n\n\nsepal.length   -0.552064\nsepal.width     0.228249\npetal.length   -1.402103\npetal.width    -1.340604\ndtype: float64\n\n\nVisualizing Class Distribution: The code print(iris_df['variety'].value_counts()) prints the count of each unique value in the variety column, showing the distribution of classes in the dataset. The sns.countplot(iris_df['variety']) function from Seaborn creates a count plot to visually represent the distribution of classes, helping to easily identify any class imbalances or differences in class frequencies.\n\n\nvariety\nSetosa        50\nVersicolor    50\nVirginica     50\nName: count, dtype: int64\n\n\nVisualizing Sepal Dimensions: The code plt.title('Comparison between sepal width and length') sets the title for the plot, while sns.scatterplot(x=iris_df['sepal.length'], y=iris_df['sepal.width']) creates a scatter plot using Seaborn to visualize the relationship between sepal length and sepal width. This visualization helps in understanding the correlation between these two features and identifying any patterns or trends in the data.\n\n\n\n\n\n\n\n\n\nEnhanced Scatter Plot with Species: The code plt.figure(figsize=(16,9)) sets the size of the plot, and plt.title('Comparison between sepal width and length on the basis of species') adds a title to the plot. The sns.scatterplot(x=iris_df['sepal.length'], y=iris_df['sepal.width'], hue=iris_df['variety'], s=50) function creates a scatter plot where each point represents sepal length and width, with different colors indicating different species. This visualization helps in comparing the sepal dimensions across species and identifying patterns or clusters in the data. The plt.show() command displays the plot.\n\n\n\n\n\n\n\n\n\nVisualizing Petal Dimensions: The code plt.title('Comparison between petal width and length') sets the title for the plot, while sns.scatterplot(x=iris_df['petal.length'], y=iris_df['petal.width']) creates a scatter plot using Seaborn to visualize the relationship between petal length and petal width. This plot helps in examining the correlation between these two features and understanding how they vary with each other in the dataset.\n\n\n\n\n\n\n\n\n\nEnhanced Scatter Plot with Species for Petal Dimensions: The code plt.figure(figsize=(10,9)) sets the size of the plot, and plt.title('Comparison between Petal width and length on the basis of species') adds a title. The sns.scatterplot(x=iris_df['petal.length'], y=iris_df['petal.width'], hue=iris_df['variety'], s=50) function creates a scatter plot where petal length and width are plotted with different colors representing species. This visualization facilitates comparison of petal dimensions across different species, helping to identify patterns or clusters. The plt.show() command displays the plot.\n\n\n\n\n\n\n\n\n\nFrom the above visualizations, we can tell that the iris-setosa species has smaller sepal length but higher width. While we see Versicolor lies in almost middle for length as well as width. While Virginica has larger sepal lengths and smaller sepal widths. We can see two separate clusters but not sure about the species so let’s bring the species into the equation as well.\nWe see that setosa has the smallest petal length as well as petal widths, while Versicolor has average petal length and petal width while the virginica species has the highest petal length as well as petal width.\nNow let’s visualize all the columns relationship using pair plots.\n\n\n&lt;Figure size 576x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nSummary of EDA: Pair plot represents the relationship between our target and the variables. We can see that the setosa species has a large difference in its characteristics when compared to the other species, it has smaller petal width and length while its sepal width is high and its sepal length is low. Similar kind of conclusions can be drawn for the other species like the Versicolor species usually have average dimensions whether it is sepal or pedal. While virginica has high pedal width and length while it has small sepal width but large sepal length. Also it is noted that Petal length and petal width are the most suitable features to classify the iris flowers in to its different varities.\nCalculating Feature Correlation: The code iris_num.corr() computes the correlation matrix for the numeric columns in the iris_num DataFrame. This matrix shows the pairwise correlation coefficients between features, helping to identify linear relationships and dependencies among them, which can be crucial for feature selection and understanding multicollinearity in the dataset.\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\n\n\n\n\nsepal.length\n1.000000\n-0.117570\n0.871754\n0.817941\n\n\nsepal.width\n-0.117570\n1.000000\n-0.428440\n-0.366126\n\n\npetal.length\n0.871754\n-0.428440\n1.000000\n0.962865\n\n\npetal.width\n0.817941\n-0.366126\n0.962865\n1.000000\n\n\n\n\n\n\n\nA visual representation of the correlation matrix is shown below.\n\n\n\n\n\n\n\n\n\nFrom the above heatmap, we see that petal length and petal width have a high correlation, petal width and sepal length have good correlation as well as petal length and sepal length have good correlations.\nFrequency distribution of feature set: Let’s see the distribution of data for the various columns of our data set.\n\n\n\n\n\n\n\n\n\nUnivariate feature analysis: The code sns.FacetGrid(iris_df, hue=\"variety\", height=5).map(sns.distplot, \"petal.width\").add_legend() creates a FacetGrid using Seaborn library to visualize the distribution of petal.width across different species in the iris_df DataFrame. The hue=\"variety\" parameter ensures that the distribution plots are colored according to the species, while height=5 sets the size of the plots. This visualization helps in analyzing the distribution and density of the petal width feature for each species, providing insights into how this feature varies across different classes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplots for Feature Analysis: The code fig, axes = plt.subplots(2, 2, figsize=(16,9)) creates a 2x2 grid of subplots with a figure size of 16x9 inches. Each sns.boxplot() function call plots the distribution of a specific feature (petal.width, petal.length, sepal.length, sepal.width) against the variety of the iris species. The orient='v' parameter specifies vertical boxplots. This visualization helps in comparing the distributions of different features across species, highlighting differences in feature ranges, central tendencies, and potential outliers. The plt.show() command displays all the plots.\n\n\n\n\n\n\n\n\n\nThe box plots describe that:\n\nThe setosa usually has smaller features with few outliers.\nThe Versicolor species has average features\nThe virginica species has the longest features widths and lengths as compared to others.\n\nWe can further see the distributions using the violin plot on our dataset\n\n\n\n\n\n\n\n\n\nThe kernel density of the data along with the full distribution of the data is shown through the violin plots. We can see the probability density of the various features. Inshort, even the basic EDA give us deeper insight about the data and give hints for classification/ regression models on feature sets."
  },
  {
    "objectID": "ML/slides-ML.html#model-identification-and-training",
    "href": "ML/slides-ML.html#model-identification-and-training",
    "title": "End-to-End Machine Learning Project",
    "section": "4. Model Identification and Training",
    "text": "4. Model Identification and Training\nBased on our data exploration, we will select a suitable classification model. Based on the Exploratory Data Analysis (EDA), we found that petal.length and petal.width are the most influential features for determining the variety of the Iris flower. To classify the Iris dataset, several classification models can be employed:\n1. Logistic Regression\nLogistic Regression is a simple yet effective classification algorithm that models the probability of a class label based on input features. It’s suitable for binary and multiclass classification problems and works well when the relationship between features and target is approximately linear.\n2. k-Nearest Neighbors (k-NN)\nk-Nearest Neighbors is a non-parametric method used for classification. It works by finding the k nearest data points to a given point and assigning the class that is most common among these neighbors. It is effective for datasets where the decision boundary is non-linear.\n3. Support Vector Machine (SVM)\nSupport Vector Machine is a powerful classification technique that works by finding the hyperplane that best separates the classes in the feature space. It is well-suited for datasets with a clear margin of separation and can handle both linear and non-linear classification tasks using kernel tricks.\n4. Decision Tree\nDecision Tree is a model that splits the data into subsets based on the value of input features, creating a tree-like structure of decisions. It is useful for handling both categorical and numerical data and provides a clear model interpretability.\n5. Random Forest\nRandom Forest is an ensemble method that combines multiple decision trees to improve classification performance. It reduces overfitting and improves accuracy by averaging the predictions from multiple trees, making it robust and effective for complex datasets.\nImporting Required Libraries\nTo perform machine learning tasks and evaluate model performance, the following libraries are imported:\n\naccuracy_score from sklearn.metrics: This function is used to compute the accuracy of the classification model by comparing the predicted labels to the true labels in the test set.\ntrain_test_split from sklearn.model_selection: This function is used to split the dataset into training and testing subsets, ensuring that the model is trained on one portion of the data and evaluated on a separate portion.\njoblib: This library is used for saving and loading Python objects efficiently, particularly for persisting trained models for future use.\n\nIn the next immediate step, we resample and split the dataset for training and testing. This can be done as follows:\nIn the next section, we will create instances of all the above mentioned classification algorithms one by one.\n1. Logistic Regression\nTo Train and Evaluate the Logistic Regression Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the LogisticRegression instance from the sklearn library. This can be done as follows.\nStep-2: Initialize the Model\nCreate an instance of the Logistic Regression model:\nStep-3” 3. Train the Model\nFit the model to the training data:\n\n\nLogisticRegression(max_iter=200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=200)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\nFor this project, we will use the RandomForestClassifier, a robust and versatile model that performs well with the Iris dataset.\n1. K-Nearest Neighbour Classifier\nTo Train and Evaluate the K-NN Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the LogisticRegression instance from the sklearn library. This can be done as follows.\nStep-2: Initialize the Model\nCreate an instance of the K-NN model:\nStep-3” 3. Train the Model\nFit the model to the training data:\n\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n3. Support Vector Machine Classifier\nTo Train and Evaluate the Logistic Regression Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the SVC instance from the sklearn library. This can be done as follows.\nStep-2: Initialize the Model\nCreate an instance of the SVC model:\nStep-3” 3. Train the Model\nFit the model to the training data:\n\n\nSVC(kernel='linear', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear', random_state=42)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n4. Decision Tree Classifier\nTo Train and Evaluate the Decision Tree Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the Decision Tree instance from the sklearn library. This can be done as follows.\nStep-2: Initialize the Model\nCreate an instance of the Decision tree model:\nStep-3” 3. Train the Model\nFit the model to the training data:\n\n\nDecisionTreeClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=42)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n5. RandonForest Classifier\nTo Train and Evaluate the RandomForest Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the Random Forest instance from the sklearn library. This can be done as follows.\nStep-2: Initialize the Model\nCreate an instance of the RandomForest model:\nStep-3: Train the Model\nFit the model to the training data:\n\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\nStep-5: Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\nFor this project, we will use the RandomForestClassifier, a robust and versatile model that performs well with the Iris dataset."
  },
  {
    "objectID": "ML/slides-ML.html#model-selection",
    "href": "ML/slides-ML.html#model-selection",
    "title": "End-to-End Machine Learning Project",
    "section": "5. Model Selection",
    "text": "5. Model Selection\nAfter training the model, we will evaluate its performance using various metrics such as accuracy and classification report. This will help us understand how well the model is performing and whether any improvements are needed. In this context, the RandomForestClassifier model is the winner. So we will select and save this model for deployment.\n\n\n['rf_model.sav']"
  },
  {
    "objectID": "ML/slides-ML.html#deployment",
    "href": "ML/slides-ML.html#deployment",
    "title": "End-to-End Machine Learning Project",
    "section": "6. Deployment",
    "text": "6. Deployment\nFinally, we will deploy our trained model using Streamlit, an open-source framework that allows us to create interactive web applications for real-time predictions. This will enable users to input flower measurements and receive predictions on the species.\nTo deploy the Random Forest Classifier model using Streamlit, we’ll need to set up several components for the complete workflow. Here’s a step-by-step guide to create the necessary files:\n1. Prepare the Environment: In this step install the streamlit library using folowing code.\npython -m pip install streamlite\n2. Create the source code to load the trained model-model.py:\nThis .py file is used to load the trained model and handle predictions.\n# model.py\n\nimport joblib\n\ndef load_model():\n    \"\"\"Load the trained Random Forest model from a file.\"\"\"\n    model = joblib.load('rf_model.sav')\n    return model\n3. Create prediction.py:\nThis .py file handles the prediction logic, using the loaded model to make predictions based on input data. The source code for this job is given below.\n\n# File name: prediction.py\nfrom model import load_model\nimport pandas as pd\n\ndef predict(data):\n    \"\"\"Predict the class of iris based on input features.\"\"\"\n    model = load_model()\n    return model.predict(data)\n4. Creating the app.py:\nThis is the main Streamlit application file. It provides the user interface for inputting data and displaying predictions.\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom prediction import predict\n\n\nst.title('Classifying Iris Flowers')\nst.markdown('Toy model to play to classify iris flowers into \\\n     (setosa, versicolor, virginica) based on their sepal/petal \\\n    and length/width.')\n\nst.header(\"Plant Features\")\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.text(\"Sepal characteristics\")\n    sepal_l = st.slider('Sepal lenght (cm)', 1.0, 8.0, 0.5)\n    sepal_w = st.slider('Sepal width (cm)', 2.0, 4.4, 0.5)\n\nwith col2:\n    st.text(\"Pepal characteristics\")\n    petal_l = st.slider('Petal lenght (cm)', 1.0, 7.0, 0.5)\n    petal_w = st.slider('Petal width (cm)', 0.1, 2.5, 0.5)\n\nst.text('')\nif st.button(\"Predict type of Iris\"):\n    result = predict(\n        np.array([[sepal_l, sepal_w, petal_l, petal_w]]))\n    st.text(result[0])\n\n\nst.text('')\nst.text('')\nst.markdown(\n    '`Create by` [Intel-Unnati](https://sijuswamy.github.io/mywebsite/) | \\\n         `Code:` [GitHub](https://github.com/sijuswamy)')\n5. Run the Streamlit App:\nNavigate to the directory containing your files and run the Streamlit app using the following command:\npython streamlit run app.py\nThis command will start a local web server and open the application in your default web browser. You can input the iris flower measurements and get the predicted variety.\nIn this project we used specific versions of the libraries. Save the following libraries in a text file- requirements.txt.\njoblib==1.3.2\nstreamlit==1.31.1\nscikit-learn==1.2.2\npandas==2.0.0\n\n\n\n\n\n\nTry it yourself!\n\n\n\nYou just open the VScode editor\nCreate a folder\nCreate a virtual environment\nActivate virtual environment\nInstall the required libraries using pip install -r requirements.txt in the powershell or command prompt\nCreate the .py file model.py and populate the content of model.py code chunk.\nCrate the .py file prediction.py and populate the content of prediction.py code chunk.\nCreate the .py file app.py and populate the content of app.py code chunk.\nRun the .py file in the command prompt or powershell using the command streamlit run app.py.\nNow you will be redirected to a localhost port and the site will be published locally.\n\n\n\n\nA screenshot of your website is shown below."
  },
  {
    "objectID": "ML/ML_lesson.html",
    "href": "ML/ML_lesson.html",
    "title": "End-to-End Machine Learning Project",
    "section": "",
    "text": "Machine learning represents a sophisticated approach to solving problems by leveraging data to inform and improve decision-making processes, contrasting sharply with traditional rule-based systems. Unlike rule-based learning, which relies on predefined rules and logic to make decisions, machine learning algorithms dynamically adapt and learn from patterns within data. This enables them to handle complex and variable inputs without explicit programming for every scenario. Essentially, machine learning is a richer computational translation of mathematical and statistical models, allowing for more flexible and accurate predictions by identifying patterns and insights that rule-based systems might overlook. A wonderful aspect of machine learning is its ability to store the learned patterns and knowledge, allowing models to be reused and refined over time for future predictions, thereby continuously enhancing their performance as more data becomes available.\nThe Classification of Machine Learning Algorithms is shown below.\n\n\n\n\n\ngraph LR\n    A[Machine Learning]\n    A --&gt; B[Supervised Learning]\n    A --&gt; C[Unsupervised Learning]\n    A --&gt; D[Reinforcement Learning]\n    \n    B --&gt; B1[Classification]\n    B --&gt; B2[Regression]\n    \n    B1 --&gt; B1a[Decision Trees]\n    B1 --&gt; B1b[Random Forests]\n    B1 --&gt; B1c[SVM]\n    B1 --&gt; B1d[Neural Networks]\n    \n    B2 --&gt; B2a[Linear Regression]\n    B2 --&gt; B2b[Polynomial Regression]\n    B2 --&gt; B2c[Ridge Regression]\n    \n    C --&gt; C1[Clustering]\n    C --&gt; C2[Dimensionality Reduction]\n    \n    C1 --&gt; C1a[K-Means]\n    C1 --&gt; C1b[Hierarchical Clustering]\n    C1 --&gt; C1c[DBSCAN]\n    \n    C2 --&gt; C2a[PCA]\n    C2 --&gt; C2b[LDA]\n    \n    D --&gt; D1[Q-Learning]\n    D --&gt; D2[Deep Q-Networks]\n    D --&gt; D3[Policy Gradient Methods]",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#introduction-to-machine-learning",
    "href": "ML/ML_lesson.html#introduction-to-machine-learning",
    "title": "End-to-End Machine Learning Project",
    "section": "",
    "text": "Machine learning represents a sophisticated approach to solving problems by leveraging data to inform and improve decision-making processes, contrasting sharply with traditional rule-based systems. Unlike rule-based learning, which relies on predefined rules and logic to make decisions, machine learning algorithms dynamically adapt and learn from patterns within data. This enables them to handle complex and variable inputs without explicit programming for every scenario. Essentially, machine learning is a richer computational translation of mathematical and statistical models, allowing for more flexible and accurate predictions by identifying patterns and insights that rule-based systems might overlook. A wonderful aspect of machine learning is its ability to store the learned patterns and knowledge, allowing models to be reused and refined over time for future predictions, thereby continuously enhancing their performance as more data becomes available.\nThe Classification of Machine Learning Algorithms is shown below.\n\n\n\n\n\ngraph LR\n    A[Machine Learning]\n    A --&gt; B[Supervised Learning]\n    A --&gt; C[Unsupervised Learning]\n    A --&gt; D[Reinforcement Learning]\n    \n    B --&gt; B1[Classification]\n    B --&gt; B2[Regression]\n    \n    B1 --&gt; B1a[Decision Trees]\n    B1 --&gt; B1b[Random Forests]\n    B1 --&gt; B1c[SVM]\n    B1 --&gt; B1d[Neural Networks]\n    \n    B2 --&gt; B2a[Linear Regression]\n    B2 --&gt; B2b[Polynomial Regression]\n    B2 --&gt; B2c[Ridge Regression]\n    \n    C --&gt; C1[Clustering]\n    C --&gt; C2[Dimensionality Reduction]\n    \n    C1 --&gt; C1a[K-Means]\n    C1 --&gt; C1b[Hierarchical Clustering]\n    C1 --&gt; C1c[DBSCAN]\n    \n    C2 --&gt; C2a[PCA]\n    C2 --&gt; C2b[LDA]\n    \n    D --&gt; D1[Q-Learning]\n    D --&gt; D2[Deep Q-Networks]\n    D --&gt; D3[Policy Gradient Methods]",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#end-to-end-machine-learning-project-classifying-the-iris-dataset",
    "href": "ML/ML_lesson.html#end-to-end-machine-learning-project-classifying-the-iris-dataset",
    "title": "End-to-End Machine Learning Project",
    "section": "End-to-End Machine Learning Project: Classifying the Iris Dataset",
    "text": "End-to-End Machine Learning Project: Classifying the Iris Dataset\nIn this project, we will walk through an end-to-end machine learning task using the Iris dataset. This comprehensive exercise will cover all stages of a machine learning pipeline, from data exploration to model deployment.",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#introduction-to-the-dataset",
    "href": "ML/ML_lesson.html#introduction-to-the-dataset",
    "title": "End-to-End Machine Learning Project",
    "section": "1. Introduction to the Dataset",
    "text": "1. Introduction to the Dataset\nThe Iris dataset is a classic dataset in machine learning, widely used for benchmarking classification algorithms. It consists of measurements from 150 iris flowers, with four features- Sepal Length, Sepal Width, Petal Length, and Petal Width. Each sample is labeled with one of three species- Iris-setosa, Iris-versicolor, and Iris-virginica.\nA sample image of iris orcid",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#objective",
    "href": "ML/ML_lesson.html#objective",
    "title": "End-to-End Machine Learning Project",
    "section": "2. Objective",
    "text": "2. Objective\nOur objective is to build a classification model that can accurately predict the species of an iris flower based on its measurements. We will explore the dataset, perform necessary preprocessing, and select an appropriate classification algorithm to achieve this goal.",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#data-exploration-and-preprocessing",
    "href": "ML/ML_lesson.html#data-exploration-and-preprocessing",
    "title": "End-to-End Machine Learning Project",
    "section": "3. Data Exploration and Preprocessing",
    "text": "3. Data Exploration and Preprocessing\n\nExploratory Data Analysis (EDA): We will begin by analyzing the dataset to understand its structure and characteristics. This includes visualizing distributions, checking for missing values, and examining class balance.\n\nIn this stage we need to load the dataset using appropriate python libraries. We want to follow a systematic approach to understand the dataset’s structure, clean the data, and gain insights. Here’s a step-by-step procedure for EDA using Python. As the first step let’s load necessary python libraries for this job.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nIn this EDA process, libraries such as pandas, seaborn, matplotlib, and scikit-learn are essential. Pandas is used for efficient data manipulation and preprocessing, allowing us to load, clean, and manage the dataset seamlessly. Seaborn and matplotlib provide advanced visualization capabilities to explore the distribution, outliers, and relationships among features, which are crucial for understanding the dataset’s structure and potential issues. Together, these libraries offer a comprehensive toolkit for conducting thorough exploratory data analysis, ensuring that the dataset is well-understood and ready for subsequent modeling.\nIn the next step, we load the Iris dataset directly from a remote URL using pandas. The code iris_df = pd.read_csv('https://raw.githubusercontent.com/sijuswamy/Model_Deployment/main/iris.csv') reads the CSV file from the specified GitHub repository and creates a DataFrame named iris_df, which contains the dataset for further analysis.\n\niris_df=pd.read_csv('https://raw.githubusercontent.com/sijuswamy/Model_Deployment/main/iris.csv')\n\nViewing the beggining Dataset: The code iris_df.head() displays the first five rows of the iris_df DataFrame, providing a quick overview of the dataset’s structure and the initial entries. We just visualize first 5 samples in the dataset as a table.\n\niris_df.head()\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nSetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nSetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nSetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nSetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nSetosa\n\n\n\n\n\n\n\nViewing the End of the Dataset: The code iris_df.tail() displays the last five rows of the iris_df DataFrame, offering insight into the final entries and the dataset’s structure at its end.\n\niris_df.tail()\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n145\n6.7\n3.0\n5.2\n2.3\nVirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nVirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nVirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nVirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nVirginica\n\n\n\n\n\n\n\nChecking the Dataset Shape: The code iris_df.shape returns a tuple representing the dimensions of the iris_df DataFrame, indicating the number of rows and columns in the dataset.\n\niris_df.shape\n\n(150, 5)\n\n\nViewing Column Names: The code iris_df.columns displays the names of all columns in the iris_df DataFrame, providing an overview of the dataset’s features and attributes.\n\niris_df.columns\n\nIndex(['sepal.length', 'sepal.width', 'petal.length', 'petal.width',\n       'variety'],\n      dtype='object')\n\n\nDataset Information: The code iris_df.info() provides a summary of the iris_df DataFrame, including the number of non-null entries, data types of each column, and memory usage, which helps assess the completeness and structure of the dataset.\n\niris_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal.length  150 non-null    float64\n 1   sepal.width   150 non-null    float64\n 2   petal.length  150 non-null    float64\n 3   petal.width   150 non-null    float64\n 4   variety       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\nStatistical Summary: The code iris_df.describe(include='all') generates a comprehensive summary of the iris_df DataFrame, including statistics for all columns, such as count, unique values, top frequency, and mean, which provides insights into the distribution and characteristics of the dataset.\n\niris_df.describe(include='all')\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n150\n\n\nunique\nNaN\nNaN\nNaN\nNaN\n3\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nSetosa\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\n50\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\nNaN\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\nNaN\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\nNaN\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\nNaN\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\nNaN\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\nNaN\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\nNaN\n\n\n\n\n\n\n\nThese basic pandas functions are most important for understand the data well. Now move on to the next level of data preparation namely Data Cleaning.\n\nData Cleaning: We will handle any missing values and ensure the data is ready for modeling. Basic preprocessing tasks will include feature scaling and normalization. Various steps in this stage is explained below.\n\nChecking for Duplicates: The code iris_df.duplicated().sum() counts the number of duplicate rows in the iris_df DataFrame, helping identify any redundancy in the dataset that may need to be addressed.\n\niris_df.duplicated().sum()\n\n1\n\n\n\n\n\n\n\n\nTip\n\n\n\nChecking for duplicates is important because duplicate rows can skew analysis, introduce bias, and affect the performance of machine learning models. By identifying and removing duplicates, we ensure that each observation is unique and that the dataset accurately represents the underlying data without redundancy.\n\n\nIdentifying Duplicate Rows: The code iris_df[iris_df.duplicated()] filters and displays the duplicate rows in the iris_df DataFrame, allowing us to inspect and address any redundancy in the dataset by showing which rows are duplicated.\n\niris_df[iris_df.duplicated()]\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n142\n5.8\n2.7\n5.1\n1.9\nVirginica\n\n\n\n\n\n\n\nChecking Class Distribution and Data Imbalance: The code iris_df['variety'].value_counts() counts the number of occurrences of each unique value in the variety column of the iris_df DataFrame, providing insight into the distribution of classes and helping to identify any class imbalances in the dataset.\n\niris_df['variety'].value_counts()\n\nvariety\nSetosa        50\nVersicolor    50\nVirginica     50\nName: count, dtype: int64\n\n\n\n\n\n\n\n\nCaution\n\n\n\nAn imbalanced dataset, where some classes are significantly underrepresented compared to others, can lead to biased model performance. It may cause the model to favor the majority class, resulting in poor predictive accuracy for the minority class and skewed evaluation metrics. Addressing class imbalance ensures that the model learns to generalize across all classes effectively, leading to more reliable and fair predictions.\n\n\nChecking for Missing Values: The code iris_df.isnull().sum(axis=0) calculates the number of missing values for each column in the iris_df DataFrame, helping to identify and address any gaps in the dataset that may need to be handled before analysis or modeling.\n\niris_df.isnull().sum(axis=0)\n\nsepal.length    0\nsepal.width     0\npetal.length    0\npetal.width     0\nvariety         0\ndtype: int64\n\n\nChecking for missing values is essential because missing data can compromise the integrity of the analysis and modeling process. By identifying columns with missing values, we can take appropriate steps to handle them—such as imputation or removal—ensuring that the dataset is complete and reliable for generating accurate insights and predictions.\nStatistical summary: Checking skewness, kurtosis, and correlation is essential for understanding data distribution and feature relationships. Skewness measures asymmetry; values between -0.5 and 0.5 indicate a fairly normal distribution, while values beyond this range suggest significant skewness. Kurtosis assesses the heaviness of tails; values close to 3 indicate a normal distribution, while values much higher or lower suggest the presence or absence of outliers, respectively. Correlation examines feature relationships, with values close to 1 or -1 indicating strong correlations that could lead to multicollinearity. Analyzing these metrics helps in identifying data transformation needs, managing outliers, and optimizing feature selection, ultimately improving model performance and reliability. Before performing the statistical operations, check for the categorical variables. If so remove them and apply statistical operations on that pruned dataset. The following code will do that.\n\n# Check if 'variety' column exists in the DataFrame\nif 'variety' in iris_df.columns:\n    removed_col = iris_df[\"variety\"]\n    iris_num = iris_df.drop('variety', axis=1)  # Use drop to remove the column and keep the rest\n    print(\"Successfully removed 'variety' column.\")\nelse:\n    print(\"Column 'variety' not found in the DataFrame.\")\n\nSuccessfully removed 'variety' column.\n\n\nChecking Skewness: The code iris_num.skew() calculates the skewness of each numeric column in the iris_num DataFrame, providing insights into the asymmetry of the data distribution. Skewness values between -0.5 and 0.5 suggest a relatively normal distribution, while values outside this range indicate potential skewness that may require transformation for better modeling.\n\niris_num.skew()\n\nsepal.length    0.314911\nsepal.width     0.318966\npetal.length   -0.274884\npetal.width    -0.102967\ndtype: float64\n\n\nChecking Kurtosis: The code iris_num.kurt() calculates the kurtosis of each numeric column in the iris_num DataFrame, which measures the “tailedness” of the data distribution. Values close to 3 suggest a distribution similar to the normal distribution, while values significantly higher or lower indicate heavy or light tails, respectively, which may point to the presence of outliers or a lack thereof.\n\niris_num.kurt()\n\nsepal.length   -0.552064\nsepal.width     0.228249\npetal.length   -1.402103\npetal.width    -1.340604\ndtype: float64\n\n\nVisualizing Class Distribution: The code print(iris_df['variety'].value_counts()) prints the count of each unique value in the variety column, showing the distribution of classes in the dataset. The sns.countplot(iris_df['variety']) function from Seaborn creates a count plot to visually represent the distribution of classes, helping to easily identify any class imbalances or differences in class frequencies.\n\nprint(iris_df['variety'].value_counts())\n\nvariety\nSetosa        50\nVersicolor    50\nVirginica     50\nName: count, dtype: int64\n\n\nVisualizing Sepal Dimensions: The code plt.title('Comparison between sepal width and length') sets the title for the plot, while sns.scatterplot(x=iris_df['sepal.length'], y=iris_df['sepal.width']) creates a scatter plot using Seaborn to visualize the relationship between sepal length and sepal width. This visualization helps in understanding the correlation between these two features and identifying any patterns or trends in the data.\n\nplt.figure(figsize=(6,5))\nplt.title('Comparison between sepal width and length')\nsns.scatterplot(x=iris_df['sepal.length'], y=iris_df['sepal.width']);\n\n\n\n\n\n\n\n\nEnhanced Scatter Plot with Species: The code plt.figure(figsize=(16,9)) sets the size of the plot, and plt.title('Comparison between sepal width and length on the basis of species') adds a title to the plot. The sns.scatterplot(x=iris_df['sepal.length'], y=iris_df['sepal.width'], hue=iris_df['variety'], s=50) function creates a scatter plot where each point represents sepal length and width, with different colors indicating different species. This visualization helps in comparing the sepal dimensions across species and identifying patterns or clusters in the data. The plt.show() command displays the plot.\n\nplt.figure(figsize=(6,5))\nplt.title('Comparison between sepal width and length on the basis of species')\nsns.scatterplot(x=iris_df['sepal.length'], y=iris_df['sepal.width'], hue = iris_df['variety'], s= 50);\nplt.show()\n\n\n\n\n\n\n\n\nVisualizing Petal Dimensions: The code plt.title('Comparison between petal width and length') sets the title for the plot, while sns.scatterplot(x=iris_df['petal.length'], y=iris_df['petal.width']) creates a scatter plot using Seaborn to visualize the relationship between petal length and petal width. This plot helps in examining the correlation between these two features and understanding how they vary with each other in the dataset.\n\nplt.figure(figsize=(6,5))\nplt.title('Comparison between petal width and length')\nsns.scatterplot(x=iris_df['petal.length'], y=iris_df['petal.width']);\n\n\n\n\n\n\n\n\nEnhanced Scatter Plot with Species for Petal Dimensions: The code plt.figure(figsize=(10,9)) sets the size of the plot, and plt.title('Comparison between Petal width and length on the basis of species') adds a title. The sns.scatterplot(x=iris_df['petal.length'], y=iris_df['petal.width'], hue=iris_df['variety'], s=50) function creates a scatter plot where petal length and width are plotted with different colors representing species. This visualization facilitates comparison of petal dimensions across different species, helping to identify patterns or clusters. The plt.show() command displays the plot.\n\nplt.figure(figsize=(6,5))\nplt.title('Comparison between Petal width and length on the basis of species')\nsns.scatterplot(x=iris_df['petal.length'], y=iris_df['petal.width'], hue = iris_df['variety'], s= 50);\nplt.show()\n\n\n\n\n\n\n\n\nFrom the above visualizations, we can tell that the iris-setosa species has smaller sepal length but higher width. While we see Versicolor lies in almost middle for length as well as width. While Virginica has larger sepal lengths and smaller sepal widths. We can see two separate clusters but not sure about the species so let’s bring the species into the equation as well.\nWe see that setosa has the smallest petal length as well as petal widths, while Versicolor has average petal length and petal width while the virginica species has the highest petal length as well as petal width.\nNow let’s visualize all the columns relationship using pair plots.\n\nplt.figure(figsize=(6,5))\nsns.pairplot(iris_df,hue=\"variety\",height=3);\n\n&lt;Figure size 576x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nSummary of EDA: Pair plot represents the relationship between our target and the variables. We can see that the setosa species has a large difference in its characteristics when compared to the other species, it has smaller petal width and length while its sepal width is high and its sepal length is low. Similar kind of conclusions can be drawn for the other species like the Versicolor species usually have average dimensions whether it is sepal or pedal. While virginica has high pedal width and length while it has small sepal width but large sepal length. Also it is noted that Petal length and petal width are the most suitable features to classify the iris flowers in to its different varities.\nCalculating Feature Correlation: The code iris_num.corr() computes the correlation matrix for the numeric columns in the iris_num DataFrame. This matrix shows the pairwise correlation coefficients between features, helping to identify linear relationships and dependencies among them, which can be crucial for feature selection and understanding multicollinearity in the dataset.\n\niris_num.corr()\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\n\n\n\n\nsepal.length\n1.000000\n-0.117570\n0.871754\n0.817941\n\n\nsepal.width\n-0.117570\n1.000000\n-0.428440\n-0.366126\n\n\npetal.length\n0.871754\n-0.428440\n1.000000\n0.962865\n\n\npetal.width\n0.817941\n-0.366126\n0.962865\n1.000000\n\n\n\n\n\n\n\nA visual representation of the correlation matrix is shown below.\n\nfig = plt.figure(figsize = (6,5))\nsns.heatmap(iris_num.corr(), cmap='Blues', annot = True);\n\n\n\n\n\n\n\n\nFrom the above heatmap, we see that petal length and petal width have a high correlation, petal width and sepal length have good correlation as well as petal length and sepal length have good correlations.\nFrequency distribution of feature set: Let’s see the distribution of data for the various columns of our data set.\n\nfig, axes = plt.subplots(2, 2, figsize=(6,5))\naxes[0,0].set_title(\"Distribution of Sepal Width\")\naxes[0,0].hist(iris_df['sepal.width'], bins=5);\naxes[0,1].set_title(\"Distribution of Sepal Length\")\naxes[0,1].hist(iris_df['sepal.length'], bins=7);\naxes[1,0].set_title(\"Distribution of Petal Width\")\naxes[1,0].hist(iris_df['petal.width'], bins=5);\naxes[1,1].set_title(\"Distribution of Petal Length\")\naxes[1,1].hist(iris_df['petal.length'], bins=6);\n\n\n\n\n\n\n\n\nUnivariate feature analysis: The code sns.FacetGrid(iris_df, hue=\"variety\", height=5).map(sns.distplot, \"petal.width\").add_legend() creates a FacetGrid using Seaborn library to visualize the distribution of petal.width across different species in the iris_df DataFrame. The hue=\"variety\" parameter ensures that the distribution plots are colored according to the species, while height=5 sets the size of the plots. This visualization helps in analyzing the distribution and density of the petal width feature for each species, providing insights into how this feature varies across different classes.\n\nsns.FacetGrid(iris_df,hue=\"variety\",height=5).map(sns.distplot,\"petal.width\").add_legend();\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(iris_df,hue=\"variety\",height=5).map(sns.distplot,\"sepal.length\").add_legend();\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(iris_df,hue=\"variety\",height=5).map(sns.distplot,\"sepal.width\").add_legend();\n\n\n\n\n\n\n\n\nBoxplots for Feature Analysis: The code fig, axes = plt.subplots(2, 2, figsize=(16,9)) creates a 2x2 grid of subplots with a figure size of 16x9 inches. Each sns.boxplot() function call plots the distribution of a specific feature (petal.width, petal.length, sepal.length, sepal.width) against the variety of the iris species. The orient='v' parameter specifies vertical boxplots. This visualization helps in comparing the distributions of different features across species, highlighting differences in feature ranges, central tendencies, and potential outliers. The plt.show() command displays all the plots.\n\nfig, axes = plt.subplots(2, 2, figsize=(6,5))\nsns.boxplot(  y=\"petal.width\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 0])\nsns.boxplot(  y=\"petal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 1])\nsns.boxplot(  y=\"sepal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 0])\nsns.boxplot(  y=\"sepal.width\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 1])\nplt.show()\n\n\n\n\n\n\n\n\nThe box plots describe that:\n\nThe setosa usually has smaller features with few outliers.\nThe Versicolor species has average features\nThe virginica species has the longest features widths and lengths as compared to others.\n\nWe can further see the distributions using the violin plot on our dataset\n\nfig, axes = plt.subplots(2, 2, figsize=(6,5))\nsns.violinplot(y=\"petal.width\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 0])\nsns.violinplot(y=\"petal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 1])\nsns.violinplot(y=\"sepal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 0])\nsns.violinplot(y=\"sepal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 1])\nplt.show()\n\n\n\n\n\n\n\n\nThe kernel density of the data along with the full distribution of the data is shown through the violin plots. We can see the probability density of the various features. Inshort, even the basic EDA give us deeper insight about the data and give hints for classification/ regression models on feature sets.",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#model-identification-and-training",
    "href": "ML/ML_lesson.html#model-identification-and-training",
    "title": "End-to-End Machine Learning Project",
    "section": "4. Model Identification and Training",
    "text": "4. Model Identification and Training\nBased on our data exploration, we will select a suitable classification model. Based on the Exploratory Data Analysis (EDA), we found that petal.length and petal.width are the most influential features for determining the variety of the Iris flower. To classify the Iris dataset, several classification models can be employed:\n\n1. Logistic Regression\nLogistic Regression is a simple yet effective classification algorithm that models the probability of a class label based on input features. It’s suitable for binary and multiclass classification problems and works well when the relationship between features and target is approximately linear.\n\n\n2. k-Nearest Neighbors (k-NN)\nk-Nearest Neighbors is a non-parametric method used for classification. It works by finding the k nearest data points to a given point and assigning the class that is most common among these neighbors. It is effective for datasets where the decision boundary is non-linear.\n\n\n3. Support Vector Machine (SVM)\nSupport Vector Machine is a powerful classification technique that works by finding the hyperplane that best separates the classes in the feature space. It is well-suited for datasets with a clear margin of separation and can handle both linear and non-linear classification tasks using kernel tricks.\n\n\n4. Decision Tree\nDecision Tree is a model that splits the data into subsets based on the value of input features, creating a tree-like structure of decisions. It is useful for handling both categorical and numerical data and provides a clear model interpretability.\n\n\n5. Random Forest\nRandom Forest is an ensemble method that combines multiple decision trees to improve classification performance. It reduces overfitting and improves accuracy by averaging the predictions from multiple trees, making it robust and effective for complex datasets.\n\n\nImporting Required Libraries\nTo perform machine learning tasks and evaluate model performance, the following libraries are imported:\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport joblib\n\n\naccuracy_score from sklearn.metrics: This function is used to compute the accuracy of the classification model by comparing the predicted labels to the true labels in the test set.\ntrain_test_split from sklearn.model_selection: This function is used to split the dataset into training and testing subsets, ensuring that the model is trained on one portion of the data and evaluated on a separate portion.\njoblib: This library is used for saving and loading Python objects efficiently, particularly for persisting trained models for future use.\n\nIn the next immediate step, we resample and split the dataset for training and testing. This can be done as follows:\n\n# random seed\nseed = 42\niris_df.sample(frac=1, random_state=seed)\n\n# selecting features and target data\nX = iris_df[['sepal.length',    'sepal.width',  'petal.length', 'petal.width']]\ny = iris_df[['variety']]\n\n# split data into train and test sets\n# 70% training and 30% test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=seed, stratify=y)\n\nIn the next section, we will create instances of all the above mentioned classification algorithms one by one.\n\n1. Logistic Regression\nTo Train and Evaluate the Logistic Regression Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the LogisticRegression instance from the sklearn library. This can be done as follows.\n\nfrom sklearn.linear_model import LogisticRegression\n\nStep-2: Initialize the Model\nCreate an instance of the Logistic Regression model:\n\nmodel = LogisticRegression(max_iter=200)\n\nStep-3” 3. Train the Model\nFit the model to the training data:\n\nmodel.fit(X_train, y_train.values.ravel())\n\nLogisticRegression(max_iter=200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=200)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\ny_pred = model.predict(X_test)\n\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\nFor this project, we will use the RandomForestClassifier, a robust and versatile model that performs well with the Iris dataset.\n\n\n1. K-Nearest Neighbour Classifier\nTo Train and Evaluate the K-NN Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the LogisticRegression instance from the sklearn library. This can be done as follows.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nStep-2: Initialize the Model\nCreate an instance of the K-NN model:\n\nmodel = KNeighborsClassifier(n_neighbors=5)\n\nStep-3” 3. Train the Model\nFit the model to the training data:\n\nmodel.fit(X_train, y_train.values.ravel())\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\ny_pred = model.predict(X_test)\n\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n\n\n3. Support Vector Machine Classifier\nTo Train and Evaluate the Logistic Regression Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the SVC instance from the sklearn library. This can be done as follows.\n\nfrom sklearn.svm import SVC\n\nStep-2: Initialize the Model\nCreate an instance of the SVC model:\n\nmodel = SVC(kernel='linear', C=1.0, random_state=seed)\n\nStep-3” 3. Train the Model\nFit the model to the training data:\n\nmodel.fit(X_train, y_train.values.ravel())\n\nSVC(kernel='linear', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear', random_state=42)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\ny_pred = model.predict(X_test)\n\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n\n\n4. Decision Tree Classifier\nTo Train and Evaluate the Decision Tree Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the Decision Tree instance from the sklearn library. This can be done as follows.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nStep-2: Initialize the Model\nCreate an instance of the Decision tree model:\n\nmodel = DecisionTreeClassifier(random_state=seed)\n\nStep-3” 3. Train the Model\nFit the model to the training data:\n\nmodel.fit(X_train, y_train.values.ravel())\n\nDecisionTreeClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=42)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\ny_pred = model.predict(X_test)\n\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n\n\n5. RandonForest Classifier\nTo Train and Evaluate the RandomForest Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the Random Forest instance from the sklearn library. This can be done as follows.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nStep-2: Initialize the Model\nCreate an instance of the RandomForest model:\n\nRf_model = RandomForestClassifier(n_estimators=100, random_state=seed)\n\nStep-3: Train the Model\nFit the model to the training data:\n\nRf_model.fit(X_train, y_train.values.ravel())\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\ny_pred = Rf_model.predict(X_test)\n\nStep-5: Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\nFor this project, we will use the RandomForestClassifier, a robust and versatile model that performs well with the Iris dataset.",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#model-selection",
    "href": "ML/ML_lesson.html#model-selection",
    "title": "End-to-End Machine Learning Project",
    "section": "5. Model Selection",
    "text": "5. Model Selection\nAfter training the model, we will evaluate its performance using various metrics such as accuracy and classification report. This will help us understand how well the model is performing and whether any improvements are needed. In this context, the RandomForestClassifier model is the winner. So we will select and save this model for deployment.\n\njoblib.dump(Rf_model, 'rf_model.sav')\n\n['rf_model.sav']",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  },
  {
    "objectID": "ML/ML_lesson.html#deployment",
    "href": "ML/ML_lesson.html#deployment",
    "title": "End-to-End Machine Learning Project",
    "section": "6. Deployment",
    "text": "6. Deployment\nFinally, we will deploy our trained model using Streamlit, an open-source framework that allows us to create interactive web applications for real-time predictions. This will enable users to input flower measurements and receive predictions on the species.\nTo deploy the Random Forest Classifier model using Streamlit, we’ll need to set up several components for the complete workflow. Here’s a step-by-step guide to create the necessary files:\n1. Prepare the Environment: In this step install the streamlit library using folowing code.\npython -m pip install streamlite\n2. Create the source code to load the trained model-model.py:\nThis .py file is used to load the trained model and handle predictions.\n# model.py\n\nimport joblib\n\ndef load_model():\n    \"\"\"Load the trained Random Forest model from a file.\"\"\"\n    model = joblib.load('rf_model.sav')\n    return model\n3. Create prediction.py:\nThis .py file handles the prediction logic, using the loaded model to make predictions based on input data. The source code for this job is given below.\n\n# File name: prediction.py\nfrom model import load_model\nimport pandas as pd\n\ndef predict(data):\n    \"\"\"Predict the class of iris based on input features.\"\"\"\n    model = load_model()\n    return model.predict(data)\n4. Creating the app.py:\nThis is the main Streamlit application file. It provides the user interface for inputting data and displaying predictions.\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom prediction import predict\n\n\nst.title('Classifying Iris Flowers')\nst.markdown('Toy model to play to classify iris flowers into \\\n     (setosa, versicolor, virginica) based on their sepal/petal \\\n    and length/width.')\n\nst.header(\"Plant Features\")\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.text(\"Sepal characteristics\")\n    sepal_l = st.slider('Sepal lenght (cm)', 1.0, 8.0, 0.5)\n    sepal_w = st.slider('Sepal width (cm)', 2.0, 4.4, 0.5)\n\nwith col2:\n    st.text(\"Pepal characteristics\")\n    petal_l = st.slider('Petal lenght (cm)', 1.0, 7.0, 0.5)\n    petal_w = st.slider('Petal width (cm)', 0.1, 2.5, 0.5)\n\nst.text('')\nif st.button(\"Predict type of Iris\"):\n    result = predict(\n        np.array([[sepal_l, sepal_w, petal_l, petal_w]]))\n    st.text(result[0])\n\n\nst.text('')\nst.text('')\nst.markdown(\n    '`Create by` [Intel-Unnati](https://sijuswamy.github.io/mywebsite/) | \\\n         `Code:` [GitHub](https://github.com/sijuswamy)')\n5. Run the Streamlit App:\nNavigate to the directory containing your files and run the Streamlit app using the following command:\npython streamlit run app.py\nThis command will start a local web server and open the application in your default web browser. You can input the iris flower measurements and get the predicted variety.\nIn this project we used specific versions of the libraries. Save the following libraries in a text file- requirements.txt.\njoblib==1.3.2\nstreamlit==1.31.1\nscikit-learn==1.2.2\npandas==2.0.0\n\n\n\n\n\n\nTry it yourself!\n\n\n\n\nYou just open the VScode editor\nCreate a folder\nCreate a virtual environment\nActivate virtual environment\nInstall the required libraries using pip install -r requirements.txt in the powershell or command prompt\nCreate the .py file model.py and populate the content of model.py code chunk.\nCrate the .py file prediction.py and populate the content of prediction.py code chunk.\nCreate the .py file app.py and populate the content of app.py code chunk.\nRun the .py file in the command prompt or powershell using the command streamlit run app.py.\nNow you will be redirected to a localhost port and the site will be published locally.\n\n\n\nA screenshot of your website is shown below.",
    "crumbs": [
      "Home",
      "ML Project"
    ]
  }
]